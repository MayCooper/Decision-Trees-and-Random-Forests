# Decision-Trees-and-Random-Forests

Decision Trees and Random Forests are both supervised machine learning algorithms used for both classification and regression tasks.

A Decision Tree is a tree-based model that uses a tree-like structure to make predictions based on the values of input features. 

The algorithm uses a greedy approach to recursively divide the feature space into smaller subsets and create a decision tree by choosing the feature that results in the most information gain at each step. The tree will continue to split on a feature until a stopping criteria is met.

The final result is a tree of decisions, where each internal node represents a test on an attribute, each branch represents the outcome of a test, and each leaf node represents a class label.

Random Forests, on the other hand, is an ensemble method which combines multiple decision trees to make predictions. It is built by creating a number of decision trees on different subsets of the data and then averaging their predictions. 

The subsets of the data are generated by random sampling of the original data set with replacement, and this is why the method is called "Random". The idea behind this method is that, by averaging the predictions of multiple trees, the final predictions will be more robust and accurate than predictions made by a single tree. 

The final predictions is made by taking the majority vote if the algorithm is used for classification, or the average of the predictions if the algorithm is used for regression.

In summary, Decision Trees are a single tree-based model, whereas Random Forests is an ensemble method that combines multiple decision trees to make predictions.
